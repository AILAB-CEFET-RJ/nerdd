{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["UcsROkE6Ys6p"],"gpuType":"T4","authorship_tag":"ABX9TyOTSaHwEcxzMghFKOpRG9dR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## IMPORT"],"metadata":{"id":"UcsROkE6Ys6p"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"wV5FPVlHQPV7","executionInfo":{"status":"ok","timestamp":1729744902368,"user_tz":180,"elapsed":7987,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"outputs":[],"source":["import os\n","import sys\n","from functools import reduce\n","from typing import Dict, List, Tuple\n","from keras.models import load_model\n","from tensorflow.python.keras import utils\n","from keras.utils import to_categorical\n","from sklearn.metrics import confusion_matrix\n","import numpy as np"]},{"cell_type":"code","source":["import glob\n","from builtins import Exception\n","from typing import List, Dict, Tuple\n","\n","import numpy as np\n","from keras.preprocessing.sequence import pad_sequences"],"metadata":{"id":"YKOvDLdOeElG","executionInfo":{"status":"ok","timestamp":1729744902369,"user_tz":180,"elapsed":4,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"I-N4OEhLQtN8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729744930798,"user_tz":180,"elapsed":28433,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}},"outputId":"ff362687-4712-45e4-e8a1-0a3b268eb472"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/Mestrado/Dissertação/NER_DD/lstm_ner')"],"metadata":{"id":"pZHBjX-YS6ph","executionInfo":{"status":"ok","timestamp":1729744930798,"user_tz":180,"elapsed":4,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from utils import data_utils"],"metadata":{"id":"P1vEU8gIYMf9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729744938719,"user_tz":180,"elapsed":7924,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}},"outputId":"1e2ef9f3-57c7-4a06-fc91-8ccc8037efb8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","source":["from keras import Sequential, Model\n","from keras.layers import Embedding, LSTM, Dropout, Dense, Reshape, Conv1D, MaxPooling1D, TimeDistributed, \\\n","    concatenate\n","import tensorflow as tf"],"metadata":{"id":"FWOMhNFxQMvv","executionInfo":{"status":"ok","timestamp":1729744938720,"user_tz":180,"elapsed":5,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["pip install Unidecode"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RtDNgnDLu6Ew","executionInfo":{"status":"ok","timestamp":1729744942987,"user_tz":180,"elapsed":4271,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}},"outputId":"53a78f97-9497-4446-daa8-014bd9ecb4ec"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting Unidecode\n","  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n","Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: Unidecode\n","Successfully installed Unidecode-1.3.8\n"]}]},{"cell_type":"code","source":["import string\n","from functools import reduce\n","\n","import nltk\n","import re\n","from unidecode import unidecode\n","from typing import List, Tuple, Dict\n","\n","nltk.download('punkt')"],"metadata":{"id":"sFt_n6aVjcLl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729744942987,"user_tz":180,"elapsed":13,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}},"outputId":"40d7b179-a8cb-42d9-80b7-aa3ffc583c71"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["#Text Utils"],"metadata":{"id":"PN_66VsAjWUh"}},{"cell_type":"code","source":["def remove_punctuations(text: str):\n","    translate_table = dict((ord(char), None) for char in string.punctuation)\n","    return text.translate(translate_table)"],"metadata":{"id":"_GB1ap3XjZR8","executionInfo":{"status":"ok","timestamp":1729744942988,"user_tz":180,"elapsed":12,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def generate_ngrams_freqdist(text, n, tokens=None):\n","    if tokens is None:\n","        tokens = nltk.word_tokenize(text)\n","    ngrams = nltk.ngrams(tokens, n)\n","    return nltk.FreqDist(ngrams), tokens"],"metadata":{"id":"NKlvp6a9jiSx","executionInfo":{"status":"ok","timestamp":1729744942988,"user_tz":180,"elapsed":12,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def score_ngrams(word_list: List[str], ngrams: nltk.FreqDist, unigrams: nltk.FreqDist, delta: float):\n","    \"\"\"\n","    Scores a list of words according to frequency distribution of each word and their ngram.\n","    This approach is based on Mikolov, Tomas, et al. \"Distributed representations of words and phrases and their\n","    compositionality.\" Advances in neural information processing systems. 2013.\n","    :param word_list: the word list to be scored\n","    :param ngrams: ngrams freqdist in which n must be the length of word_list\n","    :param unigrams: unigrams freqdist\n","    :param delta: delta is used as a discounting coefficient and prevents too many phrases consisting of very infrequent\n","     words to be formed\n","    :return: score of the word_list according to freqdist of ngrams\n","    \"\"\"\n","    assert len(word_list) == len(list(ngrams.keys())[0])\n","    # filter full of words unigrams so that it has only words that contain in word_list\n","    word_unigram_freqs = map(lambda t: t[1], filter(lambda kv: kv[0][0] in word_list, unigrams.items()))\n","    return (ngrams[tuple(word_list)] - delta) / reduce(lambda a, b: a*b, word_unigram_freqs, 1)"],"metadata":{"id":"_Yik8pUnjqSn","executionInfo":{"status":"ok","timestamp":1729744942988,"user_tz":180,"elapsed":12,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def multiple_replace(_string, replace_dict):\n","    pattern = re.compile(\"|\".join([re.escape(k) for k, v in replace_dict.items()]), re.M)\n","    return pattern.sub(lambda match: replace_dict[match.group(0)], _string)"],"metadata":{"id":"hhC0Fk7djrgj","executionInfo":{"status":"ok","timestamp":1729744942988,"user_tz":180,"elapsed":12,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def normalize_word(line):\n","    \"\"\"\n","    Transforms line to ASCII string making character translations, except some unicode characters are left because\n","    they are used in portuguese (such as ß, ä, ü, ö).\n","    \"\"\"\n","    line = line.replace(u\"„\", u\"\\\"\")\n","    line = line.lower()\n","\n","    replacements = dict(((u\"ß\", \"SZ\"), (u\"ä\", \"AE\"), (u\"ü\", \"UE\"), (u\"ö\", \"OE\")))\n","    replacements_inv = dict(zip(replacements.values(), replacements.keys()))\n","    line = multiple_replace(line, replacements)\n","    line = unidecode(line)\n","    line = multiple_replace(line, replacements_inv)\n","\n","    line = line.lower()  # unidecode might have replaced some characters, like € to upper case EUR\n","\n","    line = re.sub(\"([0-9][0-9.,]*)\", '0', line)\n","\n","    return line.strip()"],"metadata":{"id":"KLGAWsoojuNJ","executionInfo":{"status":"ok","timestamp":1729744942988,"user_tz":180,"elapsed":11,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def tokenize_sentences(sentences: List[List[Tuple[str, str]]], word_indices: Dict[str, int],\n","                       label_indices: Dict[str, int], char_level=False):\n","    unknown_idx = word_indices['UNKNOWN']\n","\n","    def tokenize(_string):\n","        if _string in word_indices:\n","            return word_indices[_string]\n","        lower = _string.lower()\n","        if lower in word_indices:\n","            return word_indices[lower]\n","        normalized = normalize_word(_string)\n","        if normalized in word_indices:\n","            return word_indices[normalized]\n","        return unknown_idx\n","\n","    def create_element(_string, label):\n","        if char_level:\n","            return [tokenize(c) for c in _string]\n","        return tokenize(_string), label_indices[label]\n","\n","    return [[create_element(word, label) for word, label in sentence] for sentence in sentences]"],"metadata":{"id":"odiYOTsAnlxL","executionInfo":{"status":"ok","timestamp":1729744942988,"user_tz":180,"elapsed":11,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# Data Utils"],"metadata":{"id":"y1ivSy-IeKS-"}},{"cell_type":"code","source":["def read_input_file(filename: str):\n","    \"\"\"\n","        Reads the input file and creates a list of sentences in which each sentence is a list of its word where the word\n","        is a 2-dim tuple, whose elements are the word itself and its label (named entity), respectively. Also creates\n","        a map of label to index.\n","\n","        Expected files have a sequence of sentences. It has one word by line in first column (in a tab-separated file)\n","        followed in second column by its label, i.e., the named entity. The sentences are separated by an empty line.\n","\n","        :param filename: Name of the file\n","        :return: List of sentences, map of label to index\n","    \"\"\"\n","    sentences = []\n","    sentence = []\n","    label2idx = {'O': 0}\n","    label_idx = 1\n","    count = 0\n","    with open(filename, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            line = line.strip()\n","            if line == \"\":\n","                if len(sentence) > 0:\n","                    sentences.append(sentence)\n","                    sentence = []\n","                continue\n","            splits = line.split('\\t')\n","            word = splits[0]\n","            label = splits[1]\n","            sentence.append((word, label))\n","            if label not in label2idx.keys():\n","                label2idx[label] = label_idx\n","                label_idx += 1\n","    return sentences, label2idx"],"metadata":{"id":"VBoX_s0HeMra","executionInfo":{"status":"ok","timestamp":1729744942988,"user_tz":180,"elapsed":11,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def create_context_windows(sentences: List[List[Tuple[int, int]]], window_size: int, padding_idx: int):\n","    \"\"\"\n","    Generates X and Y matrices. X is an array of context window (indexed according to word2Idx). Each element of the\n","    array is the context window of the word in the middle and its index in the array is the index of its label in Y\n","    matrix.\n","\n","    :param sentences: Sentences whose words and labels are already tokenized.\n","    :param window_size: How much words to the left and to the right.\n","    :param padding_idx: Index (token) for padding windows in which the main word has no enough surrounding words.\n","    :return: X and Y matrices as numpy array.\n","    \"\"\"\n","    x_matrix = []\n","    y_vector = []\n","    for sentence in sentences:\n","        for target_word_idx in range(len(sentence)):\n","            word_indices = []\n","            for wordPosition in range(target_word_idx - window_size, target_word_idx + window_size + 1):\n","                if wordPosition < 0 or wordPosition >= len(sentence):\n","                    word_indices.append(padding_idx)\n","                    continue\n","                word_idx = sentence[wordPosition][0]\n","                word_indices.append(word_idx)\n","            label_idx = sentence[target_word_idx][1]\n","            x_matrix.append(word_indices)\n","            y_vector.append(label_idx)\n","\n","\n","    return np.array(x_matrix), np.array(y_vector)"],"metadata":{"id":"9VcXlqfWeTro","executionInfo":{"status":"ok","timestamp":1729744942988,"user_tz":180,"elapsed":10,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["def read_embeddings_file(filename: str):\n","    \"\"\"\n","    Reads the embeddings file and maps its words to the index in the embeddings matrix\n","\n","    :param filename: Name of the embeddings file\n","    :return: Embeddings matrix, map of word to index\n","    \"\"\"\n","    word2idx = {}\n","    word_idx = 0\n","    char2idx = {'UNKNOWN': 0, 'PADDING': 1, 'LEFT_WORDS_PADDING': 2, 'RIGHT_WORDS_PADDING': 3}\n","    char_idx = 4\n","    embeddings = []\n","    embeddings_dim = None\n","    with open(filename, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            splits = line.strip().split(' ')\n","            if embeddings_dim is None:\n","                embeddings_dim = len(splits)\n","            elif embeddings_dim != len(splits):\n","                continue\n","            word = splits[0]\n","            for c in word:\n","                if c not in char2idx:\n","                    char2idx[c] = char_idx\n","                    char_idx += 1\n","            word2idx[word] = word_idx\n","            word_idx += 1\n","            embeddings.append(splits[1:])\n","    embeddings = np.array(embeddings, dtype=np.float32)\n","    return embeddings, word2idx, char2idx\n"],"metadata":{"id":"8Nt-GQG1eZzA","executionInfo":{"status":"ok","timestamp":1729744942988,"user_tz":180,"elapsed":10,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def create_char_context_windows(sentences: List[List[List[int]]], char2idx: Dict[str, int], word_win_size: int,\n","                                max_word_len: int):\n","    left_pad = char2idx['LEFT_WORDS_PADDING']\n","    right_pad = char2idx['RIGHT_WORDS_PADDING']\n","    inner_word_pad = char2idx['PADDING']\n","    sentences = [pad_sequences(sentences[i], maxlen=max_word_len, dtype=np.int_, value=inner_word_pad, padding='post')\n","                 for i, _ in enumerate(sentences)]\n","    sentences = [pad_sequences(sentences[i], maxlen=max_word_len + word_win_size, dtype=np.int_, value=left_pad,\n","                               padding='pre') for i, _ in enumerate(sentences)]\n","    sentences = [pad_sequences(sentences[i], maxlen=max_word_len + word_win_size * 2, dtype=np.int_, value=right_pad,\n","                               padding='post') for i, _ in enumerate(sentences)]\n","    padding_word = word_win_size * [left_pad] + max_word_len * [inner_word_pad] + word_win_size * [right_pad]\n","\n","    padded_words = []\n","    for sentence in sentences:\n","        for word_idx, word in enumerate(sentence):\n","            padded_word_window = np.array([], dtype=np.int_)\n","            for window_idx in range(word_idx - word_win_size, word_idx + word_win_size + 1):\n","                if window_idx < 0 or word_idx > len(sentence):\n","                    padded_word_window = np.append(padded_word_window, padding_word)\n","                else:\n","                    padded_word_window = np.append(padded_word_window, sentence[word_idx])\n","            padded_words.append(padded_word_window)\n","    return np.array(padded_words)\n"],"metadata":{"id":"Is2qp3Rzehmd","executionInfo":{"status":"ok","timestamp":1729744942989,"user_tz":180,"elapsed":11,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def transform_to_xy(sentences: List[List[Tuple[str, str]]], word2idx: Dict[str, int],\n","                    label2idx: Dict[str, int], word_window_size: int,\n","                    char2idx: Dict[str, int], max_word_len: int):\n","    word_indexed_sentences = tokenize_sentences(sentences, word2idx, label2idx)\n","    char_indexed_sentences = tokenize_sentences(sentences, char2idx, label2idx, char_level=True)\n","    x_word, y = create_context_windows(word_indexed_sentences, word_window_size, word2idx['PADDING'])\n","    x_char = create_char_context_windows(char_indexed_sentences, char2idx, word_window_size, max_word_len)\n","    x = [x_word, x_char]\n","    return x, y"],"metadata":{"id":"RxeMhvp-em6-","executionInfo":{"status":"ok","timestamp":1729744942989,"user_tz":180,"elapsed":10,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def load_dataset(input_data_folder: str, test_percent: float):\n","    assert 0 <= test_percent <= 1\n","    train_data, test_data, label2idx = [], [], {}\n","    for filename in glob.glob(f'{input_data_folder}/*.tsv'):\n","        print(filename)\n","        sentences, cur_lbl2idx = read_input_file(filename)\n","        if len(sentences) == 0:\n","            continue\n","        label2idx = {**label2idx, **cur_lbl2idx}\n","        test_amount = int(len(sentences) * test_percent)\n","        thresh_idx = len(sentences) - test_amount\n","        train_data += sentences[:thresh_idx]\n","        test_data += sentences[thresh_idx:]\n","\n","    return train_data, test_data, label2idx"],"metadata":{"id":"Mm4KOHTZergS","executionInfo":{"status":"ok","timestamp":1729744942989,"user_tz":180,"elapsed":10,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def save_embeddings(filename, weights, char2idx):\n","    with open(filename, 'w', encoding='utf-8') as f:\n","        for char, index in char2idx.items():\n","            line = f'{char} {\" \".join(str(item) for item in weights[index, :])}\\n'\n","            f.write(line)"],"metadata":{"id":"XzC17yV8et74","executionInfo":{"status":"ok","timestamp":1729744942989,"user_tz":180,"elapsed":10,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["# Variaves"],"metadata":{"id":"ypyUVXdjYwv5"}},{"cell_type":"code","source":["# Caminho para a pasta de dados\n","train_path = '/content/drive/MyDrive/Mestrado/Dissertação/NER_DD/lstm_ner'"],"metadata":{"id":"7yE59sIeQ7ip","executionInfo":{"status":"ok","timestamp":1729744942989,"user_tz":180,"elapsed":10,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# defining constants\n","word_embeddings_file = train_path + '/data/cbow_s50.txt'\n","input_data_folder = train_path + '/dataset'\n","model_file = train_path + '/output/model.h5'\n","char_embeddings_file = train_path + '/output/char_embeddings.txt'"],"metadata":{"id":"PLqxJF0VQZpb","executionInfo":{"status":"ok","timestamp":1729744942989,"user_tz":180,"elapsed":9,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["word_window_size = 2\n","char_window_size = 2\n","char_embeddings_dim = 20\n","dropout_rate = 0.5\n","lstm_units = 420\n","conv_num = 10\n","epochs = 1\n","test_percent = 0.2"],"metadata":{"id":"jOODFOsKZE-A","executionInfo":{"status":"ok","timestamp":1729744942989,"user_tz":180,"elapsed":8,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["# Modelo"],"metadata":{"id":"GP0SgEXJNCgR"}},{"cell_type":"code","source":["def generate_model(word_embedding_model: Sequential, char_embedding_model: Sequential, lstm_units: int, num_labels: int,\n","                   dropout_rate=.5, word_embedding_only=False, cpu_only=False):\n","    if word_embedding_only:\n","        input_layer_output = [word_embedding_model.output]\n","        hidden_layer_input_units = word_embedding_model.output_shape[2]\n","        input_layer_model = [word_embedding_model.input]\n","    else:\n","        input_layer_output = concatenate([word_embedding_model.output, char_embedding_model.output])\n","        hidden_layer_input_units = word_embedding_model.output_shape[2] + char_embedding_model.output_shape[2]\n","        input_layer_model = [word_embedding_model.input, char_embedding_model.input]\n","\n","    if cpu_only:\n","        first_lstm_net = LSTM(lstm_units, input_shape=(None, hidden_layer_input_units), return_sequences=True)\n","    else:\n","        first_lstm_net = LSTM(lstm_units, input_shape=(None, hidden_layer_input_units), return_sequences=True)\n","\n","    hidden_layer_model = Sequential()\n","    hidden_layer_model.add(first_lstm_net)\n","    hidden_layer_model.add(Dropout(dropout_rate))\n","    hidden_layer_model.add(LSTM(lstm_units) if cpu_only else LSTM(lstm_units))\n","    hidden_layer_model.add(Dense(num_labels, activation='softmax'))\n","    hidden_layer_model_output = hidden_layer_model(input_layer_output)\n","    model = Model(input_layer_model, hidden_layer_model_output)\n","    model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n","\n","    return model"],"metadata":{"id":"sUDugKldNBFB","executionInfo":{"status":"ok","timestamp":1729744942989,"user_tz":180,"elapsed":8,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["def generate_embedding(input_length: int, weights=None, vocab_size=0, embedding_dim=0):\n","    if weights is not None:\n","        vocab_size = weights.shape[0]\n","        embedding_dim = weights.shape[1]\n","        return Embedding(vocab_size, embedding_dim, input_length=input_length, weights=[weights], trainable=False)\n","    return Embedding(vocab_size, embedding_dim, input_length=input_length)"],"metadata":{"id":"96FLierNQWn-","executionInfo":{"status":"ok","timestamp":1729744942989,"user_tz":180,"elapsed":8,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["def generate_word_embedding_model(input_length: int, weights=None, vocab_size=0, embedding_dim=0):\n","    model = Sequential()\n","    model.add(generate_embedding(input_length, weights=weights, vocab_size=vocab_size, embedding_dim=embedding_dim))\n","    return model"],"metadata":{"id":"fGCQz1vBQZKl","executionInfo":{"status":"ok","timestamp":1729744942989,"user_tz":180,"elapsed":8,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["def generate_char_embedding_model(max_word_len: int, max_word_len_padded: int, word_input_len: int,\n","                                  char_embedding_dim: int, conv_num: int, char_window_size,\n","                                  vocab_size: int):\n","    char_input_len = word_input_len * max_word_len_padded\n","    model = Sequential()\n","    model.add(generate_embedding(char_input_len, vocab_size=vocab_size, embedding_dim=char_embedding_dim))\n","    model.add(Reshape((word_input_len, max_word_len_padded, char_embedding_dim)))\n","    model.add(TimeDistributed(Conv1D(conv_num, char_window_size)))\n","    model.add(TimeDistributed(MaxPooling1D(max_word_len)))\n","    model.add(Reshape((word_input_len, conv_num)))\n","    return model"],"metadata":{"id":"ukNyOYx-QafT","executionInfo":{"status":"ok","timestamp":1729744942989,"user_tz":180,"elapsed":8,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["# Main"],"metadata":{"id":"3uiceXO-NEOE"}},{"cell_type":"code","source":["def main():\n","    # loading data from files\n","    word_embeddings, word2idx, char2idx = read_embeddings_file(word_embeddings_file)\n","    max_word_len = max(map(lambda word: len(word), word2idx.keys()))\n","    train_data, test_data, label2idx = load_dataset(input_data_folder, test_percent)\n","    print('train sentences:', len(train_data))\n","    print('test sentences:', len(test_data))\n","    print(\"epochs: \", epochs)\n","    x_train, y_train = transform_to_xy(train_data, word2idx, label2idx, word_window_size,\n","                                                  char2idx, max_word_len)\n","    x_test, y_test = transform_to_xy(test_data, word2idx, label2idx, word_window_size,\n","                                                char2idx, max_word_len)\n","    num_labels = len(label2idx)\n","    # \"binarize\" labels\n","    y_train = to_categorical(y_train, num_labels)\n","    y_test = to_categorical(y_test, num_labels)\n","    # load model whether it is saved\n","    if os.path.exists(model_file):\n","        model = load_model(model_file)\n","        print(f'Model loaded from {model_file}')\n","        print(model.summary())\n","    else:\n","        # defining model\n","        word_input_length = 2 * word_window_size + 1\n","        max_word_len_padded = max_word_len + word_window_size * 2\n","        word_embedding_model = generate_word_embedding_model(word_input_length, weights=word_embeddings)\n","        char_embedding_model = generate_char_embedding_model(max_word_len, max_word_len_padded, word_input_length,\n","                                                                 char_embeddings_dim, conv_num, char_window_size,\n","                                                                 vocab_size=len(char2idx))\n","        model = generate_model(word_embedding_model, char_embedding_model, lstm_units, num_labels, dropout_rate)\n","\n","        # summarize the model\n","        print(model.summary())\n","\n","        # training model\n","        model.fit(x_train, y_train, epochs=epochs)\n","\n","        # saving embeddings\n","        embedding_layer = char_embedding_model.layers[0]\n","        weights = embedding_layer.get_weights()[0]\n","        data_utils.save_embeddings(char_embeddings_file, weights, char2idx)\n","\n","        # saving whole model\n","        # model.save(model_file)\n","\n","    # evaluating model\n","    print('x_test:')\n","    print(x_test)\n","    print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')\n","    print('y_test:')\n","    print(y_test)\n","    print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')\n","    _, accuracy, precision, recall = model.evaluate(x_test, y_test)\n","    print('Accuracy: %f' % (accuracy * 100))\n","    print('Precision: %f' % (precision * 100))\n","    print('Recall: %f' % (recall * 100))\n","    output = model.predict(x_test)\n","    testPredict = model.predict(x_test)\n","    testPredict = np.argmax(output, axis=1)\n","    y_test=np.argmax(y_test, axis=1)\n","    train_data_flat = reduce(lambda acc, cur: acc + cur, train_data, [])\n","    label_dist = {label: 0 for label in label2idx.keys()}\n","    for _, label in train_data_flat:\n","        label_dist[label] += 1\n","    print()\n","    print('####### train label distribution')\n","    print('total: %d\\n' % len(train_data_flat))\n","    for label, count in label_dist.items():\n","        print(label, count)\n","    print()\n","    cm = confusion_matrix(y_test, testPredict)\n","    print(cm)\n","    test_data_flat = reduce(lambda acc, cur: acc + cur, test_data, [])\n","    print(test_data_flat)\n","    labeled_output = label_output(output, label2idx, test_data_flat)\n","    #evaluate_model(labeled_output, test_data_flat, label2idx)"],"metadata":{"id":"DJpNf7Qfb6VF","executionInfo":{"status":"ok","timestamp":1729744942989,"user_tz":180,"elapsed":7,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["def evaluate_model(predicted: List[Tuple[str, str]], actual: List[Tuple[str, str]], label2idx: Dict[str, int]):\n","    true_pos, true_neg, false_pos, false_neg = [0] * 4\n","    labeled_metrics: Dict[str, Metrics] = {label: Metrics() for label in label2idx.keys()}\n","    confusion_matrix: Dict[str, Dict[str, int]] = {actual_label: {pred_label: 0 for pred_label in label2idx.keys()} for\n","                                                   actual_label in label2idx.keys()}\n","    not_entity_label = 'O'\n","    #------Leonidia-----\n","    #global planilha\n","    #------------------\n","    for i, pred in enumerate(predicted):\n","        pred_label, actual_label = pred[1], actual[i][1]\n","        confusion_matrix[actual_label][pred_label] += 1\n","        labeled_metrics[actual_label].actual_total += 1\n","        #-----------------------------------Leonidia--------------------------------------\n","        #get_output(pred_label, actual_label)\n","        #_________________________________________________________________________________\n","        if pred_label == actual_label == not_entity_label:\n","            true_neg += 1\n","            labeled_metrics[actual_label].true_neg += 1\n","        elif pred_label == not_entity_label:\n","            false_neg += 1\n","            labeled_metrics[actual_label].false_neg += 1\n","        elif pred_label == actual_label:\n","            true_pos += 1\n","            labeled_metrics[actual_label].true_pos += 1\n","        else:\n","            false_pos += 1\n","            labeled_metrics[pred_label].false_pos += 1\n","    #----------------Leonidia---------------------------\n","    '''dados = OrderedDict()\n","                dados.update({\"Sheet1\": planilha})\n","                save_data(\"Saida-do-modelo.xls\", dados)'''\n","    #--------------------------------------------\n","\n","    print('TP: %d\\nTN: %d\\nFP: %d\\nFN: %d' % (true_pos, true_neg, false_pos, false_neg))\n","    accuracy = (true_pos + true_neg) / len(predicted)\n","    print('Accuracy: %f' % accuracy)\n","    print(\"tru_pos value: \", true_pos)\n","    print(\"false_pos value: \", false_pos)\n","    precision = true_pos / (true_pos + false_pos)\n","    recall = true_pos / (true_pos + false_neg)\n","    f_measure = 2 * precision * recall / (precision + recall)\n","\n","    print('Precision: %f\\nRecall: %f\\nF1 score: %f' % (precision, recall, f_measure))\n","\n","    for label, metrics in labeled_metrics.items():\n","        print()\n","        print('==========>', label)\n","        print('TP: %d\\nTN: %d\\nFP: %d\\nFN: %d' % (\n","            metrics.true_pos, metrics.true_neg, metrics.false_pos, metrics.false_neg))\n","        print('Total predicted: %f' % metrics.total_predicted())\n","        print('Total actual: %f' % metrics.actual_total)\n","        print('Accuracy: %f' % metrics.accuracy())\n","        print('Precision: %f' % metrics.precision())\n","        print('Recall: %f' % metrics.recall())\n","        print('F-measure: %f' % metrics.f_measure())\n","        print()\n","\n","    # print_matrix\n","    print('Matriz de confusão (quantidades)')\n","    max_label = max(confusion_matrix.keys())\n","    print(f'{\"\".ljust(len(max_label)+3)}\\t', end='')\n","    for label in confusion_matrix.keys():\n","        print(label, end='\\t')\n","    print()\n","    for label in confusion_matrix.keys():\n","        print(label.ljust(3 + len(max_label)), end='\\t')\n","        for amount in confusion_matrix[label].values():\n","            print(str(amount).ljust(5), end='\\t')\n","        print()\n","\n","    print()\n","    # print_matrix\n","    print('Matriz de confusão (percentual)')\n","    max_label = max(confusion_matrix.keys())\n","    print(f'{\"\".ljust(len(max_label)+3)}\\t', end='')\n","    for label in confusion_matrix.keys():\n","        print(label, end='\\t')\n","    print()\n","    for label in confusion_matrix.keys():\n","        print(label.ljust(3 + len(max_label)), end='\\t')\n","        for amount in confusion_matrix[label].values():\n","            total = reduce(lambda a, b: a + b, confusion_matrix[label].values(), 0)\n","            if total == 0:\n","                amount, total = 0, 1\n","            print('%.2f%%' % (100 * amount / total), end='\\t')\n","        print()\n","\n","    return precision, recall, f_measure"],"metadata":{"id":"lyVB8QzoxORl","executionInfo":{"status":"ok","timestamp":1729744942990,"user_tz":180,"elapsed":8,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["def label_output(output: List[float], label2idx: Dict[str, int], test_data_flat: List[Tuple[str, str]]):\n","    classed_output = []\n","    for i in range(len(output)):\n","        not_entity_idx = label2idx['O']\n","        ent_prob_max = 0\n","        ent_idx = not_entity_idx\n","        for j, ent in enumerate(output[i]):\n","            if ent > ent_prob_max:\n","                ent_prob_max = ent\n","                ent_idx = j\n","        entity = [label for label, idx in label2idx.items() if idx == ent_idx][0]\n","        classed_output.append((test_data_flat[i][0], entity))\n","    return classed_output"],"metadata":{"id":"XsHmfi1RcBCD","executionInfo":{"status":"ok","timestamp":1729744942990,"user_tz":180,"elapsed":8,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    main()"],"metadata":{"id":"ReNY8Qp4cDQ_","colab":{"base_uri":"https://localhost:8080/","height":495},"executionInfo":{"status":"error","timestamp":1729744994807,"user_tz":180,"elapsed":51825,"user":{"displayName":"Gustavo Melo","userId":"05908677657003883304"}},"outputId":"cf04525a-1aa9-445d-c96f-29f174a8fc72"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Mestrado/Dissertação/NER_DD/lstm_ner/dataset/Tráfico de drogas.tsv\n","/content/drive/MyDrive/Mestrado/Dissertação/NER_DD/lstm_ner/dataset/Homicídios.tsv\n","/content/drive/MyDrive/Mestrado/Dissertação/NER_DD/lstm_ner/dataset/Armas.tsv\n","/content/drive/MyDrive/Mestrado/Dissertação/NER_DD/lstm_ner/dataset/Roubo carga- veículo.tsv\n","/content/drive/MyDrive/Mestrado/Dissertação/NER_DD/lstm_ner/dataset/Tráfico de Drogas - Armas.tsv\n","/content/drive/MyDrive/Mestrado/Dissertação/NER_DD/lstm_ner/dataset/Roubos em Geral.tsv\n","train sentences: 4685\n","test sentences: 1168\n","epochs:  1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"error","ename":"ValueError","evalue":"The layer sequential has never been called and thus has no defined output.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-c7bc734e5e35>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-29-b8ee8a2b4eb3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                                                                  \u001b[0mchar_embeddings_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_window_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                                                                  vocab_size=len(char2idx))\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embedding_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_embedding_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# summarize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-bd8f7446fca5>\u001b[0m in \u001b[0;36mgenerate_model\u001b[0;34m(word_embedding_model, char_embedding_model, lstm_units, num_labels, dropout_rate, word_embedding_only, cpu_only)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0minput_layer_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_embedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0minput_layer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_embedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_embedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mhidden_layer_input_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_embedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchar_embedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0minput_layer_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_embedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_embedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py\u001b[0m in \u001b[0;36moutput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \"\"\"\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_node_attribute_at_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_tensors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_node_attribute_at_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py\u001b[0m in \u001b[0;36m_get_node_attribute_at_index\u001b[0;34m(self, node_index, attr, attr_name)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    286\u001b[0m                 \u001b[0;34mf\"The layer {self.name} has never been called \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;34mf\"and thus has no defined {attr_name}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The layer sequential has never been called and thus has no defined output."]}]}]}
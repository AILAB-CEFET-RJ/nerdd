# calibration Subpipeline

Calibrates entity confidence scores generated by pseudolabelling outputs.

## Entrypoint

- `run_calibration.py`
- `run_calibration.py` performs **fit + apply** in a single run:
  it fits the calibration model and writes the fully calibrated corpus to `--output-jsonl`.
- `evaluate_calibration.py` performs **evaluation-only**:
  it compares RAW vs TS vs ISO and writes reliability curves and summary metrics.

## Methods

- `temperature`: global temperature scaling.
- `temperature-per-class`: one temperature per entity label.
- `isotonic`: global isotonic regression calibration.

## Label Source Modes

Recommended (real labels):
- `calibration-csv` (uses a manually labeled CSV, e.g. `data/comparacao_calibracao.csv`).

Fallback (no labeled calibration set available):
- `score-threshold`
- `quantile-bands`

## Command Example

```bash
cd dd_ner_pipeline
python3 calibration/run_calibration.py \
  --method temperature-per-class \
  --input-jsonl ./artifacts/pseudolabelling/iter01/01_predictions.jsonl \
  --output-jsonl ./artifacts/calibration/iter01/01_calibrated.jsonl \
  --stats-json ./artifacts/calibration/iter01/01_calibration_stats.json \
  --score-field score \
  --output-score-field score_calibrated \
  --preserve-original-score-field score_original \
  --labels Person,Location,Organization \
  --label-source calibration-csv \
  --calibration-csv ../data/comparacao_calibracao.csv \
  --csv-score-col Score \
  --csv-label-col Validacao \
  --log-level INFO
```

## Evaluation Example

```bash
cd dd_ner_pipeline
python3 calibration/evaluate_calibration.py \
  --calibration-csv ../data/comparacao_calibracao.csv \
  --out-dir ./artifacts/calibration/iter01/eval \
  --score-col Score \
  --label-col Validacao \
  --bins 10 \
  --cv 5
```

## Artifact Convention

Write calibration outputs under:
- `dd_ner_pipeline/artifacts/calibration/`

## Outputs

- calibrated JSONL (same records, with calibrated score field in entities)
- stats JSON with method parameters and diagnostics
- evaluation artifacts (from `evaluate_calibration.py`):
  - `metrics_summary.csv`
  - `reliability_raw.csv`
  - `reliability_ts.csv`
  - `reliability_iso.csv` (unless `--no-isotonic`)
  - `reliability_curves.png`
